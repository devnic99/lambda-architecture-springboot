package de.hs_mannheim.informatik.lambda;

import java.util.Arrays;
import java.util.List;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaPairRDD;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;

//TODO
public class WordCount {

	public static void main(String[] args) {

		SparkConf conf = new SparkConf().setAppName("wc").setMaster("local[4]");
		JavaSparkContext sc = new JavaSparkContext(conf);
		
		JavaRDD<String> tokens = sc.textFile("spark-it/src/main/resources/Faust.txt").flatMap(
					s -> Arrays.asList(s.split("\\W+")).iterator());
		
		JavaPairRDD<String, Integer> counts = tokens.mapToPair(
				token -> new Tuple2<>(token, 1)).reduceByKey((x,y) -> x+y);
		
		List<Tuple2<String, Integer>> results = counts.collect();
		results.forEach(System.out::println);
		
		sc.close();
	}

}